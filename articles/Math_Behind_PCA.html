<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>The Math Behind PCA • LearnPCA</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/united/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="The Math Behind PCA">
<meta property="og:description" content="LearnPCA">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">LearnPCA</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.1.1</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Conceptual_Intro_PCA.html">A Conceptual Introduction to PCA</a>
    </li>
    <li>
      <a href="../articles/Functions_PCA.html">A Comparison of Functions for PCA</a>
    </li>
    <li>
      <a href="../articles/Math_Behind_PCA.html">The Math Behind PCA</a>
    </li>
    <li>
      <a href="../articles/Scores_Loadings.html">Understanding Scores and Loadings</a>
    </li>
    <li>
      <a href="../articles/Start_Here.html">A Guide to the LearnPCA Package</a>
    </li>
    <li>
      <a href="../articles/Step_By_Step_PCA.html">Step-by-Step PCA</a>
    </li>
    <li>
      <a href="../articles/Visualizing_PCA_3D.html">Visualizing PCA in 3D</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/bryanhanson/LearnPCA/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>The Math Behind PCA</h1>
                        <h4 data-toc-skip class="author">David T. Harvey<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>
</h4>
                        <h4 data-toc-skip class="author">Bryan A. Hanson<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>
</h4>
            
            <h4 data-toc-skip class="date">2022-02-04</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/bryanhanson/LearnPCA/blob/HEAD/vignettes/Math_Behind_PCA.Rmd" class="external-link"><code>vignettes/Math_Behind_PCA.Rmd</code></a></small>
      <div class="hidden name"><code>Math_Behind_PCA.Rmd</code></div>

    </div>

    
    
<!-- This chunk inserts common info about all the vignettes -->
<!-- ======================================================================= -->
<div class="top-matter">
<p><em>This vignette is based upon <code>LearnPCA</code> version 0.1.1.</em></p>
<p><code>LearnPCA</code> provides the following vignettes: <a name="top-matter"></a></p>
<ul>
<li><a href="http://bryanhanson.github.io/LearnPCA/articles/Start_Here.html" class="external-link">Start Here</a></li>
<li><a href="http://bryanhanson.github.io/LearnPCA/articles/Conceptual_Intro_PCA.html" class="external-link">A Conceptual Introduction to PCA</a></li>
<li><a href="http://bryanhanson.github.io/LearnPCA/articles/Step_By_Step_PCA.html" class="external-link">Step By Step PCA</a></li>
<li><a href="http://bryanhanson.github.io/LearnPCA/articles//Scores_Loadings.html" class="external-link">Understanding Scores &amp; Loadings</a></li>
<li><a href="http://bryanhanson.github.io/LearnPCA/articles/Visualizing_PCA_3D.html" class="external-link">Visualizing PCA in 3D</a></li>
<li><a href="http://bryanhanson.github.io/LearnPCA/articles/Math_Behind_PCA.html" class="external-link">The Math Behind PCA</a></li>
<li><a href="http://bryanhanson.github.io/LearnPCA/articles/Functions_PCA.html" class="external-link">PCA Functions</a></li>
<li>To access the vignettes with <code>R</code>, simply type <code>browseVignettes("LearnPCA")</code> to get a clickable list in a browser window.</li>
</ul>
<p><strong>Vignettes are available in both pdf (on CRAN) and html formats (at Github). </strong></p>
</div>
<!-- ======================================================================= -->
<p>In this vignette we’ll look closely at how the data reduction step in PCA is actually done. This vignette is intended for those who really want to dig deep. It’s helpful if you know something about matrix manipulations, but we work hard to keep the level of the material accessible to those who are just learning.</p>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>If you have read the <a href="#top-matter">Step By Step PCA</a> vignette, you know that the first steps in PCA are:</p>
<ol style="list-style-type: decimal">
<li>Center the data by subtracting the column means from the columns.</li>
<li>Optionally, scale the data column-wise.</li>
<li>Carry out the reduction step, typically using <code>prcomp</code>.</li>
</ol>
<p>For a data matrix with <span class="math inline">\(n\)</span> rows of observations/samples and <span class="math inline">\(p\)</span> variables/features, the results are the</p>
<ul>
<li>scores matrix with <span class="math inline">\(n\)</span> rows and <span class="math inline">\(p\)</span> columns, where each column corresponds to a principal component and the values are the scores, namely the positions of the samples in the new coordinate system.</li>
<li>loadings matrix with <span class="math inline">\(p\)</span> rows and <span class="math inline">\(p\)</span> columns, which represent the contributions of each variable to each principal component.</li>
</ul>
<p>In the <a href="#top-matter">Step By Step PCA</a> vignette we also showed how to reconstruct or approximate the original data set by multiplying the scores by the transpose of the loadings.</p>
<p>In Figure <a href="#fig:PCA-Matrices">1</a> we show one way to represent the relationships between the original data matrix (<span class="math inline">\(\mathbf{X}\)</span>), the loadings matrix (<span class="math inline">\(\mathbf{L}\)</span>) and the scores matrix (<span class="math inline">\(\mathbf{S}\)</span>).</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:PCA-Matrices"></span>
<img src="Graphics/PCA_Matrices.png" alt="One way to look at the matrix algebra behind PCA. Reconstruction of the data matrix $\mathbf{X}$ is achieved by multiplying the score matrix ($\mathbf{S}$) by the transpose of the loadings matrix ($\mathbf{L}$).  The method of matrix multiplication is symbolized in the red-dotted outlines: Each element of row $i$ of the scores matrix is multiplied by the corresponding element of column $j$ of the transposed loadings.  These results are summed to give a single entry in the original data matrix $\mathbf{X}_{ij}$." width="75%"><p class="caption">
Figure 1: One way to look at the matrix algebra behind PCA. Reconstruction of the data matrix <span class="math inline">\(\mathbf{X}\)</span> is achieved by multiplying the score matrix (<span class="math inline">\(\mathbf{S}\)</span>) by the transpose of the loadings matrix (<span class="math inline">\(\mathbf{L}\)</span>). The method of matrix multiplication is symbolized in the red-dotted outlines: Each element of row <span class="math inline">\(i\)</span> of the scores matrix is multiplied by the corresponding element of column <span class="math inline">\(j\)</span> of the transposed loadings. These results are summed to give a single entry in the original data matrix <span class="math inline">\(\mathbf{X}_{ij}\)</span>.
</p>
</div>
<p>Figure <a href="#fig:PCA-Matrices">1</a> demonstrates that if we multiply the scores matrix (<span class="math inline">\(\mathbf{S}\)</span>) by the transpose of the loadings matrix (<span class="math inline">\(\mathbf{L^T}\)</span>), we get back the original data matrix (<span class="math inline">\(\mathbf{X}\)</span>). When we start however, all we have is the original data matrix, how do we get the other two matrices? If we think of this as an algebra problem, we seem to be missing some variables; computing the loadings and scores matrices seems like it would be impossible, as there is not enough information. However, this is not a algebra problem, it is a <em>linear</em> algebra problem (linear algebra being the study of matrices). It <em>is</em> possible to determine the answer, even though we seem to be missing information, as we shall see shortly. The key is in something called matrix decompositions.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
</div>
<div class="section level2">
<h2 id="matrix-decompositions">Matrix Decompositions<a class="anchor" aria-label="anchor" href="#matrix-decompositions"></a>
</h2>
<p>In a moment we are going to look at two matrix decompositions in detail, the singular value decomposition (SVD) and the eigenvalue decomposition. These decompositions are representative of roughly a dozen <a href="https://en.wikipedia.org/wiki/Matrix_decomposition" class="external-link">matrix decompositions</a>. A matrix decomposition or factorization breaks a matrix into pieces in such a way as to extract information and solve problems. The SVD is probably the most powerful decomposition there is – we will make extensive use of this insightful <a href="https://twitter.com/WomenInStat/status/1285610321747611653" class="external-link">Twitter thread</a> by Dr. Daniela Witten of the University of Washington. We also use this excellent <a href="https://stats.stackexchange.com/a/134283/26909" class="external-link">Cross Validated answer by amoeba</a>.</p>
<p>A short note however, before we dig deeper. Since we are using a computer to solve this problem, we need to keep in mind that using a computer is not quite the same as solving a problem using pencil and paper. On the computer, we usually have choices of algorithms to solve problems. Some algorithms are more robust than others. Algorithms need to take into account edge cases where the computation can become unstable. For instance, computers can only store numbers to a certain level of accuracy: when is “very small” actually zero in practice? We need to know this so we don’t try to divide by zero. This is only one example of problems that can arise when using a computer to calculate values.</p>
<p>Finally, the two decompositions we are going to look at have something in common. Our approach will be to explain each without reference to the other, as this facilitates digestion and understanding of each (it doesn’t seem fair to require you to understand the 2nd one that you haven’t read while trying to understand the first one). Then, if you are still with us, we’ll look at what they have in common.</p>
</div>
<div class="section level2">
<h2 id="the-svd-decomposition">The SVD Decomposition<a class="anchor" aria-label="anchor" href="#the-svd-decomposition"></a>
</h2>
<p>We’ll start from the original data matrix <span class="math inline">\(\mathbf{X}\)</span> which has samples in rows and measured variables in columns. Let’s assume that we have column-centered the matrix. The SVD decomposition breaks this matrix <span class="math inline">\(\mathbf{X}\)</span> into three matrices (dimensions in parentheses):<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p><span class="math display" id="eq:svd1">\[\begin{equation}
\tag{1}
\mathbf{X}_{(n \ \times \ p)} = \mathbf{U}_{(n \ \times \ p)}\mathbf{D}_{(p \ \times \ p)}\mathbf{V}_{(p \ \times \ p)}^T
\end{equation}\]</span></p>
<p>And here’s the equation without matrix dimensions. Remember that <span class="math inline">\(\mathbf{V}^T\)</span> means “take the transpose” of <span class="math inline">\(\mathbf{V}\)</span>, interchanging rows and columns.</p>
<p><span class="math display" id="eq:svd2">\[\begin{equation}
\tag{2}
\mathbf{X} = \mathbf{U}\mathbf{D}\mathbf{V}^T
\end{equation}\]</span></p>
<p>The equation above is where we’d like to end up. How can we get there? Let’s start with <em>What are these matrices</em>?</p>
<ul>
<li>
<span class="math inline">\(\mathbf{X}\)</span> contains the original data</li>
<li>The columns of <span class="math inline">\(\mathbf{U}\)</span> are vectors giving the principal axes. These define the new coordinate system.</li>
<li>The scores can be obtained by <span class="math inline">\(\mathbf{X}\mathbf{V}\)</span>; scores are the projections of the data on the principal axes.</li>
<li>
<span class="math inline">\(\mathbf{D}\)</span> is a diagonal matrix, which means all non-diagonal elements are zero. The diagonal contains positive values sorted from largest to smallest. These are called the singular values.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>
</li>
<li>The columns of <span class="math inline">\(\mathbf{V}\)</span> are the PCA loadings</li>
</ul>
<p>In addition, <span class="math inline">\(\mathbf{U}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> are semi-orthogonal matrices,<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> which means that when pre-multiplied by their transpose one gets the identity matrix:<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p><span class="math display" id="eq:svd3">\[\begin{equation}
\tag{3}
\mathbf{U}^{T}\mathbf{U} = \mathbf{I}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:svd4">\[\begin{equation}
\tag{4}
\mathbf{V}^{T}\mathbf{V} = \mathbf{I}
\end{equation}\]</span></p>
<p>We’ll use this fact to great advantage when we implement a simple version of SVD in a moment.</p>
</div>
<div class="section level2">
<h2 id="a-simple-implementation-of-svd">A Simple Implementation of SVD<a class="anchor" aria-label="anchor" href="#a-simple-implementation-of-svd"></a>
</h2>
<p>Now that we know what these matrices are, we can look into how to compute them. As mentioned earlier, the algorithm is everything here. As a simple example, we’ll look at an approach called “power iteration.” This is by no means the best approach, but it is simple enough that we can understand the idea. First, let’s generate some random data. In this simple example we are only going to compute the first principal component, so <code>V</code> is a vector, not a matrix (one can still do matrix multiplication with a vector, we just treat it as a “row vector” or a “column vector”).<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> Note that the dimensions of the variables are chosen so that we can matrix multiply them (they are <em>conformable</em> matrices).</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">30</span><span class="op">)</span>
<span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">100</span><span class="op">*</span><span class="fl">50</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">50</span><span class="op">)</span>
<span class="va">V</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">50</span><span class="op">)</span></code></pre></div>
<p>Next, we’ll use the built-in function <code>svd</code> to compute the “official” answer for comparison to our results.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">X_svd</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/svd.html" class="external-link">svd</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span></code></pre></div>
<p>We’ll do a simple iterative calculation that computes the values of <code>U</code> and <code>V</code>. The loop runs for 50 iterations, and as it does the values of <code>U</code> and <code>V</code> are continuously updated and get closer to the actual answer.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw">for</span> <span class="op">(</span><span class="va">iter</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">50</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">U</span> <span class="op">&lt;-</span> <span class="va">X</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="va">V</span> <span class="co"># Step 1</span>
  <span class="va">U</span> <span class="op">&lt;-</span> <span class="va">U</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">U</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="co"># Step 2</span>
  <span class="va">V</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="va">U</span> <span class="co"># Step 3</span>
  <span class="va">V</span> <span class="op">&lt;-</span> <span class="va">V</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">V</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="co"># Step 4</span>
 <span class="kw">if</span> <span class="op">(</span><span class="op">(</span><span class="va">iter</span> <span class="op"><a href="https://rdrr.io/r/base/Arithmetic.html" class="external-link">%%</a></span> <span class="fl">10</span><span class="op">)</span> <span class="op">==</span> <span class="fl">0L</span><span class="op">)</span> <span class="op">{</span> <span class="co"># report every 10 steps; print the correlation between</span>
   <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"\nIteration"</span>, <span class="va">iter</span>, <span class="st">"\n"</span><span class="op">)</span> <span class="co"># the current U or V and the actual values from SVD</span>
   <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"\tcor with V:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/sprintf.html" class="external-link">sprintf</a></span><span class="op">(</span><span class="st">"%f"</span>, <span class="fu"><a href="https://rdrr.io/r/stats/cor.html" class="external-link">cor</a></span><span class="op">(</span><span class="va">X_svd</span><span class="op">$</span><span class="va">v</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>, <span class="va">V</span><span class="op">)</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span>
   <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"\tcor with U:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/sprintf.html" class="external-link">sprintf</a></span><span class="op">(</span><span class="st">"%f"</span>, <span class="fu"><a href="https://rdrr.io/r/stats/cor.html" class="external-link">cor</a></span><span class="op">(</span><span class="va">X_svd</span><span class="op">$</span><span class="va">u</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>, <span class="va">U</span><span class="op">)</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span>
   <span class="op">}</span>
<span class="op">}</span></code></pre></div>
<pre><code><span class="co">## </span>
<span class="co">## Iteration 10 </span>
<span class="co">##  cor with V: -0.931153 </span>
<span class="co">##  cor with U: -0.917383 </span>
<span class="co">## </span>
<span class="co">## Iteration 20 </span>
<span class="co">##  cor with V: -0.998758 </span>
<span class="co">##  cor with U: -0.998542 </span>
<span class="co">## </span>
<span class="co">## Iteration 30 </span>
<span class="co">##  cor with V: -0.999969 </span>
<span class="co">##  cor with U: -0.999965 </span>
<span class="co">## </span>
<span class="co">## Iteration 40 </span>
<span class="co">##  cor with V: -0.999999 </span>
<span class="co">##  cor with U: -0.999999 </span>
<span class="co">## </span>
<span class="co">## Iteration 50 </span>
<span class="co">##  cor with V: -1.000000 </span>
<span class="co">##  cor with U: -1.000000</span></code></pre>
<p>Notice that there is no <span class="math inline">\(\mathbf{D}\)</span> matrix in this calculation. This is because we are only calculating a single principal component, and therefore in this case <span class="math inline">\(\mathbf{D}\)</span> is a scalar constant. We can drop it from the calculation. With that simplification, we can look at each step.</p>
<div class="section level3">
<h3 id="step-1">Step 1<a class="anchor" aria-label="anchor" href="#step-1"></a>
</h3>
<p>The first step is to multiply the data matrix <code>X</code> by the initial estimate for <code>V</code> (remember at each iteration the estimate gets better and better). How does this relate to Equation <a href="#eq:svd2">(2)</a>? If we drop <span class="math inline">\(\mathbf{D}\)</span> from equation <a href="#eq:svd2">(2)</a> we have:</p>
<p><span class="math display" id="eq:svd5">\[\begin{equation}
\tag{5}
\mathbf{X} = \mathbf{U}\mathbf{V}^T
\end{equation}\]</span></p>
<p>If we right multiply both sides by <span class="math inline">\(\mathbf{V}\)</span> we have:</p>
<p><span class="math display" id="eq:svd6a">\[\begin{equation}
\tag{6}
\mathbf{X}\mathbf{V} = \mathbf{U}\mathbf{V}^T\mathbf{V}
\end{equation}\]</span></p>
<p>which evaluates to:</p>
<p><span class="math display" id="eq:svd6b">\[\begin{equation}
\tag{7}
\mathbf{X}\mathbf{V} = \mathbf{U}\mathbf{I} = \mathbf{U}
\end{equation}\]</span></p>
<p>because <span class="math inline">\(\mathbf{V}\)</span> is a semi-orthogonal matrix. This is the line in the code.</p>
</div>
<div class="section level3">
<h3 id="step-2">Step 2<a class="anchor" aria-label="anchor" href="#step-2"></a>
</h3>
<p>In this step we normalize (regularize, or scale) the estimate of <code>U</code>, by dividing by the square root of the sum of the squared values in <code>U</code>.<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> This has the practical effect of keeping the values in <code>U</code> from becoming incredibly large and possibly overflowing memory.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a></p>
</div>
<div class="section level3">
<h3 id="step-3">Step 3<a class="anchor" aria-label="anchor" href="#step-3"></a>
</h3>
<p>Here we update our estimate of <code>V</code>. Similar to Step 1, we can rearrange Equation <a href="#eq:svd2">(2)</a>, this time by dropping <span class="math inline">\(\mathbf{D}\)</span>, pre-multiplying both sides by <span class="math inline">\(\mathbf{U}^T\)</span> to give an identity matrix which drops out, and then transposing both sides:</p>
<p><span class="math display" id="eq:svd7">\[\begin{equation}
\tag{8}
\mathbf{U}^T\mathbf{X} = \mathbf{U}^T\mathbf{U}\mathbf{V}^T
\end{equation}\]</span></p>
<p><span class="math display" id="eq:svd8">\[\begin{equation}
\tag{9}
\mathbf{U}^T\mathbf{X} = \mathbf{I}\mathbf{V}^T
\end{equation}\]</span></p>
<p><span class="math display" id="eq:svd9">\[\begin{equation}
\tag{10}
\mathbf{U}^T\mathbf{X} = \mathbf{V}^T
\end{equation}\]</span></p>
<p><span class="math display" id="eq:svd10">\[\begin{equation}
\tag{11}
(\mathbf{U}^T\mathbf{X})^T = (\mathbf{V}^T)^T
\end{equation}\]</span></p>
<p><span class="math display" id="eq:svd11">\[\begin{equation}
\tag{12}
\mathbf{X}^T\mathbf{U} = \mathbf{V}
\end{equation}\]</span></p>
<p>which is the operation we see in the code snippet.<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a></p>
</div>
<div class="section level3">
<h3 id="step-4">Step 4<a class="anchor" aria-label="anchor" href="#step-4"></a>
</h3>
<p>Step 4 is the same operation as in Step 2, but on <code>V</code>.</p>
</div>
<div class="section level3">
<h3 id="overall">Overall<a class="anchor" aria-label="anchor" href="#overall"></a>
</h3>
<p>Essentially what this algorithm is doing is alterating between the two calculations (for <code>U</code>, steps 1 &amp; 2, then for <code>V</code> steps 3 &amp; 4), with <code>X</code> constant. At each iteration these estimates improve, moving from the initial random value of <code>V</code> towards the best answer for both <code>V</code>and <code>U</code>.</p>
</div>
<div class="section level3">
<h3 id="reporting">Reporting<a class="anchor" aria-label="anchor" href="#reporting"></a>
</h3>
<p>You’ll notice that the code snippet above has a few lines to report the progress of the calculation. Every 10 steps the correlation between the current value of <code>V</code> with the official answer contained in <code>X_svd$v</code> is displayed (and the same for <code>U</code>). As you can see from the output the correlation is not bad after 10 iterations and only improves with more iterations. We report the correlation because the signs of the power iteration answers may vary from those computed by <code>svd</code>.<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></p>
</div>
<div class="section level3">
<h3 id="comparison-to-the-answer-from-svd">Comparison to the Answer from <code>svd</code><a class="anchor" aria-label="anchor" href="#comparison-to-the-answer-from-svd"></a>
</h3>
<p>Let’s compare the absolute values of the two different answers:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="va">V</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="va">X_svd</span><span class="op">$</span><span class="va">v</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code><span class="co">## [1] 4.735975e-06</span></code></pre>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="va">U</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="va">X_svd</span><span class="op">$</span><span class="va">u</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code><span class="co">## [1] 2.752052e-06</span></code></pre>
<p>As you can see, the values are essentially the same except for sign.</p>
</div>
<div class="section level3">
<h3 id="comparison-to-the-answer-from-prcomp">Comparison to the Answer from <code>prcomp</code><a class="anchor" aria-label="anchor" href="#comparison-to-the-answer-from-prcomp"></a>
</h3>
<p>We have seen that our estimate of <code>U</code> and <code>V</code> compared well to the results from the function <code>svd</code>. In practice users would probably use <code>prcomp</code> for PCA. So let’s compare to the results from <code>prcomp</code>.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">PCA</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html" class="external-link">prcomp</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="va">X</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="va">V</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="va">PCA</span><span class="op">$</span><span class="va">x</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="co"># compare the scores</span></code></pre></div>
<pre><code><span class="co">## [1] 0.004591257</span></code></pre>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="va">V</span><span class="op">)</span><span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="va">PCA</span><span class="op">$</span><span class="va">rotation</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="co"># compare the loadings</span></code></pre></div>
<pre><code><span class="co">## [1] 0.0001339306</span></code></pre>
<p>These mean differences are very small but not quite as good as using <code>svd</code> directly.</p>
<p>Notice that we have worked through the SVD without mentioning eigen-anything. That was one of our goals. Now let’s take a different point of view.</p>
</div>
</div>
<div class="section level2">
<h2 id="the-eigen-decomposition">The Eigen Decomposition<a class="anchor" aria-label="anchor" href="#the-eigen-decomposition"></a>
</h2>
<p>The eigen decomposition is another way to decompose a data matrix. This decomposition breaks the data matrix <span class="math inline">\(\mathbf{X}\)</span> into two matrices.<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a> Again, let’s assume <span class="math inline">\(\mathbf{X}\)</span> has been centered.</p>
<p><span class="math display" id="eq:ed1">\[\begin{equation}
\tag{13}
\mathbf{X}_{(n \ \times \ n)} = \mathbf{Q}_{(n \ \times \ n)}\mathbf{\Lambda}_{(n \ \times \ n)}\mathbf{Q}_{(n \ \times \ n)}^{T}
\end{equation}\]</span></p>
<p>And here’s the equation without matrix dimensions.<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a></p>
<p><span class="math display" id="eq:ed2">\[\begin{equation}
\tag{14}
\mathbf{X} = \mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{T}
\end{equation}\]</span></p>
<p>The equation above is where we’d like to end up; it looks like a variation on the equation for SVD. How can we get there? Once again, let’s take inventory of the matrices in the equation.</p>
<ul>
<li>
<span class="math inline">\(\mathbf{X}\)</span> is the original data matrix. Notice the dimensions are <span class="math inline">\(n \times n\)</span>, unlike in SVD. In other words, it is a square matrix. Your data is not square you say? Eigen decomposition requires an square matrix. Fortunately, there’s a simple fix for this. We can work instead with the covariance matrix, which is square: <span class="math inline">\(\mathbf{X}^T\mathbf{X}/(n - 1)\)</span>. This retains all the structure of the original data.<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a>
</li>
<li>
<span class="math inline">\(\mathbf{Q}\)</span> is square matrix that whose columns will contain the <em>eigenvectors</em>. <span class="math inline">\(\mathbf{Q}\)</span> is also an orthogonal matrix (discussed earlier in the SVD section). Notice that <span class="math inline">\(\mathbf{Q}\)</span> appears twice in <a href="#eq:ed2">(14)</a>, the second time as its transpose.</li>
<li>
<span class="math inline">\(\mathbf{\Lambda}\)</span> (upper case Greek letter Lambda) is a diagonal matrix, very similar in function to <span class="math inline">\(\mathbf{D}\)</span> in SVD. All non-diagonal elements are zero. The diagonal contains values sorted from largest to smallest. These are called the <em>eigenvalues</em>.</li>
</ul>
<p>So what are eigenvectors and eigenvalues? The eigenvalues are related to the amount of variance explained for each principal component. The eigenvectors are the principal axes, which as we have seen constitute a new coordinate system for looking at the data (see the <a href="#top-matter">Visualizing PCA in 3D</a> vignette for details). If we post-multiply the original data by the eigenvectors in <span class="math inline">\(\mathbf{Q}\)</span> we get the scores:</p>
<p><span class="math display" id="eq:ed4">\[\begin{equation}
\tag{15}
\mathbf{X}\mathbf{Q} = scores
\end{equation}\]</span></p>
<p>And the loadings are simply the eigenvectors in <span class="math inline">\(\mathbf{Q}\)</span>.</p>
<div class="section level3">
<h3 id="a-simple-implementation-of-the-eigen-decomposition">A Simple Implementation of the Eigen Decomposition<a class="anchor" aria-label="anchor" href="#a-simple-implementation-of-the-eigen-decomposition"></a>
</h3>
<p>As we did for <code>SVD</code>, we can use a power iteration to compute estimates for the first eigenvector.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">30</span><span class="op">)</span>
<span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">100</span><span class="op">*</span><span class="fl">50</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">50</span><span class="op">)</span>
<span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html" class="external-link">cor</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>
<span class="va">Q</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">50</span><span class="op">)</span></code></pre></div>
<p>Next, we’ll use the built-in function <code>eigen</code> to compute the “official” answer for comparison to our results.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">X_eig</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/eigen.html" class="external-link">eigen</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span></code></pre></div>
<p>A simple iterative process as we did for SVD will continuously update the value of <code>Q</code>.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw">for</span> <span class="op">(</span><span class="va">iter</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">50</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">Q</span> <span class="op">&lt;-</span> <span class="va">X</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="va">Q</span> <span class="co"># Step 1</span>
  <span class="va">Q</span> <span class="op">&lt;-</span> <span class="va">Q</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">Q</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="co"># Step 2</span>

 <span class="kw">if</span> <span class="op">(</span><span class="op">(</span><span class="va">iter</span> <span class="op"><a href="https://rdrr.io/r/base/Arithmetic.html" class="external-link">%%</a></span> <span class="fl">5</span><span class="op">)</span> <span class="op">==</span> <span class="fl">0L</span><span class="op">)</span> <span class="op">{</span> <span class="co"># report every 5 steps; print the correlation between</span>
   <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"\nIteration"</span>, <span class="va">iter</span>, <span class="st">"\n"</span><span class="op">)</span> <span class="co"># the current Q and the actual values from SVD</span>
   <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"\tcor with Q:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/sprintf.html" class="external-link">sprintf</a></span><span class="op">(</span><span class="st">"%f"</span>, <span class="fu"><a href="https://rdrr.io/r/stats/cor.html" class="external-link">cor</a></span><span class="op">(</span><span class="va">X_eig</span><span class="op">$</span><span class="va">vectors</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>, <span class="va">Q</span><span class="op">)</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span>
   <span class="op">}</span>
<span class="op">}</span></code></pre></div>
<pre><code><span class="co">## </span>
<span class="co">## Iteration 5 </span>
<span class="co">##  cor with Q: 0.669676 </span>
<span class="co">## </span>
<span class="co">## Iteration 10 </span>
<span class="co">##  cor with Q: 0.933282 </span>
<span class="co">## </span>
<span class="co">## Iteration 15 </span>
<span class="co">##  cor with Q: 0.989643 </span>
<span class="co">## </span>
<span class="co">## Iteration 20 </span>
<span class="co">##  cor with Q: 0.998310 </span>
<span class="co">## </span>
<span class="co">## Iteration 25 </span>
<span class="co">##  cor with Q: 0.999698 </span>
<span class="co">## </span>
<span class="co">## Iteration 30 </span>
<span class="co">##  cor with Q: 0.999942 </span>
<span class="co">## </span>
<span class="co">## Iteration 35 </span>
<span class="co">##  cor with Q: 0.999988 </span>
<span class="co">## </span>
<span class="co">## Iteration 40 </span>
<span class="co">##  cor with Q: 0.999997 </span>
<span class="co">## </span>
<span class="co">## Iteration 45 </span>
<span class="co">##  cor with Q: 0.999999 </span>
<span class="co">## </span>
<span class="co">## Iteration 50 </span>
<span class="co">##  cor with Q: 1.000000</span></code></pre>
<p>How does the final estimate for <code>Q</code> compare to the official answer? We can check our result as before:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="va">Q</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="va">X_eig</span><span class="op">$</span><span class="va">vectors</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="co"># check the loadings</span></code></pre></div>
<pre><code><span class="co">## [1] -8.0837e-06</span></code></pre>
<p>Good work by the power iteration!</p>
</div>
</div>
<div class="section level2">
<h2 id="the-relationship-between-svd-and-eigen-decomposition">The Relationship Between SVD and Eigen Decomposition<a class="anchor" aria-label="anchor" href="#the-relationship-between-svd-and-eigen-decomposition"></a>
</h2>
<p>It’s apparent that SVD and the eigen decomposition have a lot in common. The <code>R</code> function <code>prcomp</code> uses the <code>svd</code> function “under the hood”, and the function <code>princomp</code> uses <code>eigen</code> under the hood. The vignette <a href="#top-matter">PCA Functions</a> goes into greater detail about the similarities and differences between these two decompositions as implemented in <code>R</code>.</p>
<div class="section level3">
<h3 id="singular-values-vs-eigenvalues">Singular Values vs Eigenvalues<a class="anchor" aria-label="anchor" href="#singular-values-vs-eigenvalues"></a>
</h3>
<p>We’ve talked about “values” in the context of each decomposition. Is this terminology accidental, or is there a relationship? If you square the singular values from SVD and divide by <span class="math inline">\(n -1\)</span>, you get the eigenvalues. Here “diagonal” means take the diagonal elements of the matrix (which would be a vector of values):</p>
<p><span class="math display" id="eq:r1">\[\begin{equation}
\tag{16}
(diagonal(\mathbf{D}))^2/(n - 1) = diagonal(\mathbf{\Lambda})
\end{equation}\]</span></p>
<p>Either of these “values” can be used to compute the amount of variance explained by each principal component. Details are in the <a href="#top-matter">PCA Functions</a> vignette.</p>
</div>
<div class="section level3">
<h3 id="pros-and-cons">Pros and Cons<a class="anchor" aria-label="anchor" href="#pros-and-cons"></a>
</h3>
<ul>
<li>
<code>svd</code> can handle rectangular matrices <span class="math inline">\(n \times p\)</span> where <span class="math inline">\(n \ne p\)</span>. Either <span class="math inline">\(n &gt; p\)</span> or <span class="math inline">\(n &lt; p\)</span> is acceptable. On the other hand, <code>eigen</code> must have <span class="math inline">\(n = p\)</span>. <code>prcomp</code> wraps <code>eigen</code> and helps the user convert their raw data matrix into a square matrix.</li>
</ul>
<!-- Insert refer_to_works_consulted document -->
</div>
</div>
<div class="section level2">
<h2 id="works-consulted">Works Consulted<a class="anchor" aria-label="anchor" href="#works-consulted"></a>
</h2>
<p>In addition to references and links in this document, please see the <a href="http://bryanhanson.github.io/LearnPCA/articles/Start_Here.html#works-consulted" class="external-link">Works Consulted</a> section of the <em>Start Here</em> vignette for general background.</p>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references">
<div id="ref-Savov2020">
<p>Savov, Ivan. 2020. <em>No Bullshit Guide to Linear Algebra</em>. 2nd ed. minireference.com.</p>
</div>
<div id="ref-Singh2014">
<p>Singh, Kuldeep. 2014. <em>Linear Algebra Step by Step</em>. Oxford University Press.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>Professor of Chemistry &amp; Biochemistry, DePauw University, Greencastle IN USA., <a href="mailto:harvey@depauw.edu" class="email">harvey@depauw.edu</a><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Professor Emeritus of Chemistry &amp; Biochemistry, DePauw University, Greencastle IN USA., <a href="mailto:hanson@depauw.edu" class="email">hanson@depauw.edu</a><a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>If you need an introduction to linear algebra, there are many good books, but we particularly recommend <span class="citation">Singh (<a href="#ref-Singh2014" role="doc-biblioref">2014</a>)</span> or <span class="citation">Savov (<a href="#ref-Savov2020" role="doc-biblioref">2020</a>)</span>.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>This treatment is the “compact SVD” <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition" class="external-link">case</a>.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>The singular values are used to calculate the variance explained by each principal component. We’ll have more to say about that later.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>For semi-orthogonal matrices (or orthogonal matrices for that matter), <span class="math inline">\(A^TA = AA^T = I\)</span>. Semi-orthogonal matrices are rectangular. For non-rectangular orthogonal matrices, if <span class="math inline">\(n &gt; p\)</span>, the dot product of any column with itself is 1, and the dot product of any column with a different column is zero. If <span class="math inline">\(n &lt; p\)</span>, then it is the rows rather than the columns that are relevant. See the <a href="https://en.wikipedia.org/wiki/Semi-orthogonal_matrix" class="external-link">Wikipedia article</a>.<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>The identity matrix <span class="math inline">\(\mathbf{I}\)</span> is a square matrix with ones on the diagonal and zeros everywhere else. The identity matrix can pre- or post-multiply any other matrix and not affect that matrix.<a href="#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>When refering to the mathematical equations, we’ll use <span class="math inline">\(\mathbf{X}\)</span>, but when referencing the values we compute, we’ll use <code>X</code>.<a href="#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>This is the <span class="math inline">\(L^2\)</span> or Euclidean norm, generally interpreted as a length.<a href="#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>As the values grow larger and larger, first we lose precision and eventually the numbers become too big to store.<a href="#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p>The operation from equation <a href="#eq:svd10">(11)</a> to <a href="#eq:svd11">(12)</a> is based upon the following property in linear algebra: <span class="math inline">\((AB)^T\)</span> = <span class="math inline">\(B^TA^T\)</span>.<a href="#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>From <code><a href="https://rdrr.io/r/stats/prcomp.html" class="external-link">?prcomp</a></code> “The signs of the columns of the rotation matrix are arbitrary, and so may differ between different programs for PCA, and even between different builds of R.” From <code><a href="https://rdrr.io/r/stats/princomp.html" class="external-link">?princomp</a></code> “The signs of the columns of the loadings and scores are arbitrary, and so may differ between different programs for PCA, and even between different builds of R: fix_sign = TRUE alleviates that.” We discuss the origin of the different signs in more detail in <a href="#top-matter">PCA Functions vignette</a>.<a href="#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>Eigenvalues and eigenvectors are extremely important in linear algebra. The concept is usually introduced in the simpler form <span class="math inline">\(Au = \lambda u\)</span>.<a href="#fnref13" class="footnote-back">↩︎</a></p></li>
<li id="fn14"><p>One also sees this written <span class="math inline">\(\mathbf{X} = \mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{-1}\)</span>. This is equivalent because for orthogonal matrices <span class="math inline">\(\mathbf{A}^T = \mathbf{A}^{-1}\)</span>.<a href="#fnref14" class="footnote-back">↩︎</a></p></li>
<li id="fn15"><p>We can also use the correlation matrix, which is evaluated via the same formula. For covariance, one centers the raw data columns. For correlation, one centers the raw data and then scales them by their standard deviation.<a href="#fnref15" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Bryan A. Hanson, David T. Harvey.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.2.9000.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
